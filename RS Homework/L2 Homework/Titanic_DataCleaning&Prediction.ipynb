{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     accuracy  precision     recall         F1\n",
      "DecisionTree        74.887892  66.666667  65.060241  65.853659\n",
      "LogisticRegression  82.062780  72.839506  76.623377  74.683544\n",
      "LDA                 81.165919  71.604938  75.324675  73.417722\n",
      "NaiveBayes          80.269058  74.074074  72.289157  73.170732\n",
      "SVM                 84.304933  69.135802  84.848485  76.190476\n",
      "KNN                 81.165919  70.370370  76.000000  73.076923\n",
      "AdaBoost            82.511211  74.074074  76.923077  75.471698\n",
      "XGBoost             81.614350  67.901235  78.571429  72.847682\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=120, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.8158861498688678\n",
      "Generation 2 - Current best internal CV score: 0.8158861498688678\n",
      "Generation 3 - Current best internal CV score: 0.8158861498688678\n",
      "Generation 4 - Current best internal CV score: 0.8158861498688678\n",
      "Generation 5 - Current best internal CV score: 0.8218454095437598\n",
      "\n",
      "Best pipeline: XGBClassifier(input_matrix, learning_rate=0.1, max_depth=7, min_child_weight=2, n_estimators=100, nthread=1, subsample=0.7500000000000001)\n",
      "     PassengerId  Pclass                                          Name  \\\n",
      "0            892       3                              Kelly, Mr. James   \n",
      "1            893       3              Wilkes, Mrs. James (Ellen Needs)   \n",
      "2            894       2                     Myles, Mr. Thomas Francis   \n",
      "3            895       3                              Wirz, Mr. Albert   \n",
      "4            896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)   \n",
      "..           ...     ...                                           ...   \n",
      "413         1305       3                            Spector, Mr. Woolf   \n",
      "414         1306       1                  Oliva y Ocana, Dona. Fermina   \n",
      "415         1307       3                  Saether, Mr. Simon Sivertsen   \n",
      "416         1308       3                           Ware, Mr. Frederick   \n",
      "417         1309       3                      Peter, Master. Michael J   \n",
      "\n",
      "        Sex   Age  SibSp  Parch              Ticket      Fare Embarked  \\\n",
      "0      male  34.5      0      0              330911    7.8292        Q   \n",
      "1    female  47.0      1      0              363272    7.0000        S   \n",
      "2      male  62.0      0      0              240276    9.6875        Q   \n",
      "3      male  27.0      0      0              315154    8.6625        S   \n",
      "4    female  22.0      1      1             3101298   12.2875        S   \n",
      "..      ...   ...    ...    ...                 ...       ...      ...   \n",
      "413    male  27.0      0      0           A.5. 3236    8.0500        S   \n",
      "414  female  39.0      0      0            PC 17758  108.9000        C   \n",
      "415    male  38.5      0      0  SOTON/O.Q. 3101262    7.2500        S   \n",
      "416    male  27.0      0      0              359309    8.0500        S   \n",
      "417    male  27.0      1      1                2668   22.3583        C   \n",
      "\n",
      "    Survived  \n",
      "0         遇难  \n",
      "1         遇难  \n",
      "2         遇难  \n",
      "3         遇难  \n",
      "4         遇难  \n",
      "..       ...  \n",
      "413       遇难  \n",
      "414       幸存  \n",
      "415       遇难  \n",
      "416       遇难  \n",
      "417       遇难  \n",
      "\n",
      "[418 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "class titanic():\n",
    "    def __init__(self, train_path, test_path):\n",
    "        self.train = pd.read_csv(train_path)\n",
    "        self.test = pd.read_csv(test_path)\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y, self.predict_x = self.data_clean()\n",
    "        self.result = self.training()\n",
    "        self.evaluation()\n",
    "        self.predict()\n",
    "\n",
    "    def data_clean(self):\n",
    "        # 使用中位数填充训练集和测试集的‘Age'列\n",
    "        self.train['Age'].fillna(self.train['Age'].median(), inplace = True)\n",
    "        self.test['Age'].fillna(self.test['Age'].median(), inplace = True)\n",
    "        # 使用中位数填充测试集的'Fare'列\n",
    "        self.test['Fare'].fillna(self.test['Fare'].median(), inplace = True)\n",
    "        # 使用出现最多的港口'S'来填充训练集的'Embarked'列\n",
    "        self.train['Embarked'].fillna('S', inplace = True)\n",
    "        # 删除训练集和测试集中数据量过少的‘Cabin'列\n",
    "        self.train.drop(['Cabin'], axis = 1, inplace = True)\n",
    "        self.test.drop(['Cabin'], axis = 1, inplace = True)\n",
    "        # 定义模型特征\n",
    "        features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "        # 定义训练集特征数据\n",
    "        train_data = self.train[features]\n",
    "        # 定义训练集标签\n",
    "        train_target = self.train['Survived']\n",
    "        # 定义预测数据特征\n",
    "        predict_data = self.test[features]\n",
    "        # 提取特征,产生非稀疏矩阵\n",
    "        vec = DictVectorizer(sparse = False)\n",
    "        # 将训练集数据与预测数据进行预处理，转化为特征向量\n",
    "        train_data = vec.fit_transform(train_data.to_dict(orient = 'record'))\n",
    "        predict_data = vec.fit_transform(predict_data.to_dict(orient = 'record'))\n",
    "        # 将训练集数据再次细分为训练集和测试集，此处并不包括预测集，细分目的是为了便于评估模型结果\n",
    "        train_x, test_x, train_y, test_y = train_test_split(train_data, train_target, test_size = 0.25)\n",
    "        # Z-score标准化训练集、测试集以及预测集\n",
    "        sd = preprocessing.StandardScaler()\n",
    "        train_sd_x = sd.fit_transform(train_x)\n",
    "        test_sd_x = sd.transform(test_x)\n",
    "        predict_sd_x = sd.transform(predict_data)\n",
    "        return train_sd_x, test_sd_x, train_y, test_y, predict_sd_x\n",
    "\n",
    "    def training(self):\n",
    "        # 初始化训练结果\n",
    "        result_all = []\n",
    "        # 使用CART决策树算法\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        dt = DecisionTreeClassifier()\n",
    "        dt.fit(self.train_x, self.train_y)\n",
    "        result_dt = dt.predict(self.test_x)\n",
    "        result_all.append(result_dt)\n",
    "        # 使用逻辑回归算法\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        lr = LogisticRegression(solver = 'lbfgs')\n",
    "        lr.fit(self.train_x, self.train_y)\n",
    "        result_lr = lr.predict(self.test_x)\n",
    "        result_all.append(result_lr)\n",
    "        # 使用LDA算法\n",
    "        from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(self.train_x, self.train_y)\n",
    "        result_lda = lda.predict(self.test_x)\n",
    "        result_all.append(result_lda)\n",
    "        # 使用朴素贝叶斯算法\n",
    "        from sklearn.naive_bayes import BernoulliNB\n",
    "        nb = BernoulliNB()\n",
    "        nb.fit(self.train_x, self.train_y)\n",
    "        result_nb = nb.predict(self.test_x)\n",
    "        result_all.append(result_nb)\n",
    "        # 使用SVM算法\n",
    "        from sklearn import svm\n",
    "        svec = svm.SVC()\n",
    "        svec.fit(self.train_x, self.train_y)\n",
    "        result_svec = svec.predict(self.test_x)\n",
    "        result_all.append(result_svec)\n",
    "        # 使用KNN算法\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn.fit(self.train_x, self.train_y)\n",
    "        result_knn = knn.predict(self.test_x)\n",
    "        result_all.append(result_knn)\n",
    "        # 使用AdaBoost算法\n",
    "        from sklearn.ensemble import AdaBoostClassifier\n",
    "        ada = AdaBoostClassifier()\n",
    "        ada.fit(self.train_x, self.train_y)\n",
    "        result_ada = ada.predict(self.test_x)\n",
    "        result_all.append(result_ada)\n",
    "        # 使用XGBoost算法\n",
    "        from xgboost import XGBClassifier\n",
    "        xg = XGBClassifier()\n",
    "        xg.fit(self.train_x, self.train_y)\n",
    "        result_xg = xg.predict(self.test_x)\n",
    "        result_all.append(result_xg)\n",
    "        # 返回所有算法训练结果\n",
    "        return result_all\n",
    "\n",
    "    def evaluation(self):\n",
    "        # 初始化评估结果\n",
    "        all = []\n",
    "        # 评估上述每种算法的指标，将其放入dict中\n",
    "        for i in self.result:\n",
    "            eval = {}\n",
    "            eval['accuracy'] = metrics.accuracy_score(i, self.test_y) * 100\n",
    "            eval['precision'] = metrics.precision_score(i, self.test_y) * 100\n",
    "            eval['recall'] = metrics.recall_score(i, self.test_y) * 100\n",
    "            eval['F1'] = metrics.f1_score(i, self.test_y) * 100\n",
    "            # 将算法评估指标的dict放入评估结果list\n",
    "            all.append(eval)\n",
    "        # 将评估结果list转化为DataFrame展示\n",
    "        print (pd.DataFrame(all, index=['DecisionTree', 'LogisticRegression', 'LDA', 'NaiveBayes', 'SVM', 'KNN', 'AdaBoost', 'XGBoost']))\n",
    "\n",
    "    def predict(self):\n",
    "        # 使用TPOT预测数据\n",
    "        from tpot import TPOTClassifier\n",
    "        tp = TPOTClassifier(generations = 5, population_size = 20, verbosity = 2)\n",
    "        tp.fit(self.train_x, self.train_y)\n",
    "        result_tp = tp.predict(self.predict_x)\n",
    "        # 将预测结果添加至预测集数据\n",
    "        self.test['Survived'] = result_tp\n",
    "        # 输出预测结果\n",
    "        self.test['Survived'].replace([0, 1], ['遇难', '幸存'], inplace = True)\n",
    "        predict_result = self.test\n",
    "        print (predict_result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    titanic = titanic('C:/Users/Administrator/Desktop/RS/L2Data/titan/train.csv', \n",
    "                      'C:/Users/Administrator/Desktop/RS/L2Data/titan/test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
